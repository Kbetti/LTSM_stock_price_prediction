# -*- coding: utf-8 -*-
"""LTSM_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zY7TcE-XoQBAu50_oV4NQJDdeEyjV5TG
"""

# ===================================================
# ===================================================
# Passo 1: Instalação e Configurações Iniciais
# ===================================================
!pip install --upgrade yfinance

import os
import yfinance as yf
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

# Configuração para utilizar GPU (caso disponível)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Torch executando no dispositivo: {device}")
import yfinance as yf
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

# Configuração para utilizar GPU (caso disponível)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Torch executando no dispositivo: {device}")

# ===================================================
# Passo 2: Coleta e Pré-Processamento de Dados
# ===================================================
# Baixar os dados históricos das ações
symbol = 'AAPL'  # Simbolo ação
start_date = '2020-01-01'
end_date = '2025-04-30'

data = yf.download(symbol, start=start_date, end=end_date)
prices = data[['Close']]  # Fechamento

# Normalizar os dados
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(prices.values)

# Dividir os dados em treino (80%) e teste (20%)
training_size = int(len(scaled_data) * 0.8)
train_data, test_data = scaled_data[:training_size], scaled_data[training_size:]

# Criar os datasets para o PyTorch
def create_dataset(dataset, time_step=50):
    X, Y = [], []
    for i in range(len(dataset) - time_step - 1):
        X.append(dataset[i:(i + time_step), 0])
        Y.append(dataset[i + time_step, 0])
    return np.array(X), np.array(Y)

time_step = 10
X_train, y_train = create_dataset(train_data, time_step)
X_test, y_test = create_dataset(test_data, time_step)

# Transformar os dados em tensores para PyTorch
X_train = torch.tensor(X_train.reshape(X_train.shape[0], time_step, 1), dtype=torch.float32).to(device)
y_train = torch.tensor(y_train, dtype=torch.float32).to(device)
X_test = torch.tensor(X_test.reshape(X_test.shape[0], time_step, 1), dtype=torch.float32).to(device)
y_test = torch.tensor(y_test, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Commented out IPython magic to ensure Python compatibility.
# ===================================================
# Passo 3: Modelo e Treinamento com Logs no TensorBoard
# ===================================================
# Definir o modelo LSTM
class StockPriceLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):
        super(StockPriceLSTM, self).__init__()

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Usar a última saída
        return out

# Ajuste os hiperparâmetros
hidden_dim = 128  #  número de neurônios
num_layers = 3    #  camadas
model = StockPriceLSTM(input_dim=1, hidden_dim=hidden_dim, output_dim=1, num_layers=num_layers).to(device)
print(f"Modelo transferido para o dispositivo: {next(model.parameters()).device}")

# Definir função de perda e otimizador
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay= 0)

# Configurar logs do TensorBoard
log_dir = os.path.join("logs", datetime.now().strftime("%Y%m%d-%H%M%S"))
writer = SummaryWriter(log_dir)

# Função de treinamento
def train_model_with_logs(model, train_loader, test_loader, criterion, optimizer, epochs=50):
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0

        # Loop de Treinamento
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()  # Zerar gradientes
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch.view(-1, 1))
            loss.backward()  # Backpropagation
            optimizer.step()  # Atualizar parâmetros
            train_loss += loss.item()

        # Avaliação no conjunto de teste
        model.eval()
        test_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                y_pred = model(X_batch)
                loss = criterion(y_pred, y_batch.view(-1, 1))
                test_loss += loss.item()

        # Logs no TensorBoard
        train_loss_avg = train_loss / len(train_loader)
        test_loss_avg = test_loss / len(test_loader)

        writer.add_scalar("Loss/Train", train_loss_avg, epoch)
        writer.add_scalar("Loss/Test", test_loss_avg, epoch)

        print(f"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_loss_avg:.4f}, Test Loss: {test_loss_avg:.4f}")

# Treinar modelo
epochs = 100
train_model_with_logs(model, train_loader, test_loader, criterion, optimizer, epochs)

# ===================================================
# Passo 4: Visualizar Logs no TensorBoard no Colab
# ===================================================
# Exibir os logs do TensorBoard diretamente no Colab
# %load_ext tensorboard
# %tensorboard --logdir logs

# ===================================================
# Passo 5: Salvar o Modelo
# ===================================================
torch.save(model.state_dict(), "lstm_stock_model.pth")
print("Modelo salvo com sucesso!")

# Carregar o modelo salvo
# Use the same hidden_dim and num_layers as when the model was saved
model = StockPriceLSTM(input_dim=1, hidden_dim=128, output_dim=1, num_layers=3).to(device)
model.load_state_dict(torch.load("lstm_stock_model.pth"))
model.eval()  # Colocar o modelo em modo de avaliação
print("Modelo carregado com sucesso!")

# Fazer predições no conjunto de teste
with torch.no_grad():
    y_pred = model(X_test)  # Predições normalizadas
    y_pred = y_pred.cpu().numpy()  # Converter para um array do NumPy
    y_true = y_test.cpu().numpy()  # Valores reais

# Inverter a normalização para trazer os valores previstos para sua faixa original
y_pred_original = scaler.inverse_transform(y_pred)
y_true_original = scaler.inverse_transform(y_true.reshape(-1, 1))

# Fazer predições no conjunto de teste
with torch.no_grad():
    y_pred = model(X_test)  # Predições normalizadas
    y_pred = y_pred.cpu().numpy()  # Converter para um array do NumPy
    y_true = y_test.cpu().numpy()  # Valores reais

# Inverter a normalização para trazer os valores previstos para sua faixa original
y_pred_original = scaler.inverse_transform(y_pred)
y_true_original = scaler.inverse_transform(y_true.reshape(-1, 1))

import matplotlib.pyplot as plt

# Plotar as predições versus os valores reais
plt.figure(figsize=(12, 6))
plt.plot(y_true_original, label="Valor Real", color="blue")
plt.plot(y_pred_original, label="Valor Previsto", color="orange")
plt.title("Predição x Real para Preço de Ações")
plt.xlabel("Dias de Teste")
plt.ylabel("Preço (em escala original)")
plt.legend()
plt.show()

from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_true_original, y_pred_original)
print(f"Erro Médio Absoluto (MAE): {mae:.2f}")

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_true_original, y_pred_original)
print(f"Erro Quadrático Médio (MSE): {mse:.2f}")

rmse = np.sqrt(mse)
print(f"Raiz do Erro Quadrático Médio (RMSE): {rmse:.2f}")

def calcular_mape(y_true_original, y_pred_original):
    """
    Calcula o MAPE utilizando o sklearn (de forma customizada).
    :param y_real: Valores reais (array ou lista)
    :param y_previsto: Valores previstos (array ou lista)
    :return: MAPE em percentual
    """

    # Mean Absolute Error (MAE), mas dividido pelos valores reais
    mape = np.mean(np.abs((y_true_original - y_pred_original) / (y_true_original + 1e-10))) * 100
    return mape # Adicionado o retorno do valor MAPE

# Cálculo do MAPE
mape = calcular_mape(y_true_original, y_pred_original)
print(f"O MAPE das previsões é: {mape:.2f}%")